## Context
The approach to purchase pattern analysis is a crucial aspect of understanding consumer behavior and optimizing marketing strategies. This analysis involves examining how customers interact with products, their purchasing habits. By leveraging data analytics, businesses can identify trends, preferences, and potential areas for improvement in their sales processes. This analysis can lead to more targeted marketing campaigns, improved customer satisfaction, and ultimately, increased sales. This analysis will feed on the [summary of revenue analysis](summary_of_revenue_analysis.md). This summary should also provide the rationale for the criteria used in the analysis. 

## Approach

1. **Data Collection**: The data developed in the previous revenue segmentation study is used to develop the data for the purchase pattern analysis. 
2. **Data Preparation**: The initial analysis is performed for the Sao Paulo region, focusing on the frequently purchased products. The analysis for RJ and MG should be performed later and should be similar. For the temporal analysis, the data is aggregated by week. If a different cadence is needed, it can be adjusted later.  The next analysis will focus on the geographic distribution of the purchases, which will be based on the data from the previous analysis.
3. **Data Analysis**: The following are the key methodological considerations in preparing the data for the analysis:
    - **High Dimensional Data**: The data is high dimensional, so working directly in the feature space will require dimensionality reduction techniques. The approach used here is to use a similarity matrix approach for clustering. This eliminates the need for dimensionality reduction. The size of the data in raw input space 52 weeks * 4300 (approximately) The similarity matrix will be of size 52 * 52, which is manageable. Using a factorization approach - SVD, separable NMF etc., usually require paramter tuning and more assumption verification. If there is work in the feature space, that readers are aware of that are not tedious in terms of parameter selection and assumption verification, please let me know. Note, I am not saying working in the feature space is a bad idea, it is just that the similarity matrix approach is more straightforward and does not require additional parameter tuning. Given the descriptive nature of the analysis, this approach is preferred.
    - **Choice of Similarity Metric**: The following similarity metrics are considered:
        - **Cosine Similarity**: This metric captures week to week similarity in items that are purchased. It captures weeks that have similar itmes purchased. The quantiy of items purchased is not considered, only the presence or absence of items in that weeks purchases is relevant. So this gives us a view of weeks where similar products are purchased in the store. This can give insights to those involved in demand planning, inventory management, marketing and price strategy.
        - **Euclidean Distance**: This metric captures the distance between weeks in terms of the length of the vector of items purchased. The length of the vector is related to the weekly sales revenue from the frequently purchased items. The inverse of the distance is used to compute the euclidean similarity. This metric is useful to group weeks with similar sales volume of frequently purchased items. This can give insights to those involved in revenue planning and marketing
 4. **Descriptive Analytics**: The clustering is performed using the similarity matrix. Spectral clustering is used because it is a good choice for clustering with the similarity matrix. As a graph based clustering method, it has been applied to many high dimensional data problems. The clustering is performed using the cosine similarity metric. The number of clusters is based on evaluating the heat map for the cosine similarity matrix and a histogram of the euclidean similarity for clustering the respective similarity matrices. Please see the code for the details.

4. **Visualization**: The results of the clustering are visualized using a violin plot. The violin plot shows the distribution of the clusters and the number of weeks in each cluster. This gives a good overview of the clusters and their distribution. The heat map of the cosine similarity matrix is also visualized to show the similarity between weeks. A scatter plot of weekly revenue as a function of the week of the year with the cluster overlaid shows how each of the similarity metrics provides a different temporal view of weekly sales revenue.

5. **Interpretation**: When viewing the analysis, please keep in mind that the analysis uses a vector of weekly sales revenue for the frequently purchased items. ** This is not a conventional time series analysis**. To do that you need to analyze the univariate weekly sales revenue. If you want to analyze canonical problems in time series analysis, such as motif discovery, trend analysis, seasonality, etc., you should use the univariate weekly sales revenue. This analysis is more about describing the multivariate amounts of frequently purchased items in the store over the weeks of 2017. The analysis is descriptive in nature and does not make any assumptions about the data. It is a good way to get an overview of the purchase patterns in the store and can be used to inform further analysis.
6. **Next Steps**: The next steps in the analysis are to perform the geographic distribution of the purchases, which will be based on the data from the previous analysis. This will provide insights into the geographic distribution of the purchases and can be used to inform further analysis. The analysis can also be extended to other regions, such as RJ and MG, to get a more comprehensive view of the purchase patterns in Brazil.

NOTE: The approach taken here is to use human judgement to segment the raw transaction data into a form suitable for analysis. In the process ML and statistical methods are used to analyze the data. This is a top down approach to the analysis. The bottom up approach is to use ML and statistical methods to segment the raw transaction data into a form suitable for analysis. For example you can use a descision tree to segment the data into clusters and then analyze the clusters. You can adjust the number of clusters by choosing the granularity of the segmentation you need - how much similarity, alternatively, the diversity you want to capture in this clusters. This may be useful in some approaches, for example, if you know apriori all that matters is revenue, then you can use your favorite flavor of decision tree (I have used CART) - in fact other methods are possible too to segment daily transaction data into segments. This is a nice approach too. The choice of approach depends on the need. I might do the bottom up approach later, but for now I am using the top down approach. 